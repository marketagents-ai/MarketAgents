# orchestrator_config.yaml

num_agents: 4
max_rounds: 1
environment_order:
  - group_chat
  - research
tool_mode: true
agent_config:
  knowledge_base: "hamlet_kb"
  use_llm: true
llm_configs:
    - name: "gpt-4o"
      model: "gpt-4o-mini"
      client: "openai"
      max_tokens: 1024
      temperature: 0.5
      use_cache: true
    - name: "claude"
      model: "claude-3-5-sonnet-latest"
      client: "anthropic"
      max_tokens: 4096
      temperature: 0.5
      use_cache: true
    - name: "hermes"
      model: "openai/NousResearch/Hermes-3-Llama-3.1-8B"
      client: "litellm"
      max_tokens: 4096
      temperature: 0.5
      use_cache: true
    - name: "deepseek"
      model: "openai/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
      client: "litellm"
      max_tokens: 4096
      temperature: 0.5
      use_cache: true
    - name: "qwq"
      model: "openai/Qwen/QwQ-32B-Preview"
      client: "litellm"
      max_tokens: 4096
      temperature: 0.5
      use_cache: true
    - name: "qwen"
      model: "openai/Qwen/Qwen2.5-7B-Instruct"
      client: "litellm"
      max_tokens: 4096
      temperature: 0.5
      use_cache: true

environment_configs:
  group_chat:
    name: "group_chat"
    api_url: "http://localhost:8002"
  #  initial_topic: "Initial Market Discussion"
    initial_topic: "Hamlet's famous 'To be or not to be' soliloquy. Use emoji's and colloquial language for discussion"
    sub_rounds: 2
    group_size: 4
  research:
    name: "literary_research"
    api_url: "http://localhost:8003"
#    initial_topic: "Market Analysis"
    initial_topic: "Hamlet's famous 'To be or not to be' soliloquy', record your aha moments with emojis"
    sub_rounds: 2
    group_size: 4
    schema_model: "LiteraryAnalysis" 
protocol: "acl_message"
database_config:
  db_host: "localhost"
  db_port: "5433"

request_limits:
  openai:
    max_requests_per_minute: 500
    max_tokens_per_minute: 40000
  anthropic:
    max_requests_per_minute: 300
  vllm:
    max_requests_per_minute: 150
    max_tokens_per_minute: 50000
  litellm:
    max_requests_per_minute: 100
    max_tokens_per_minute: 35000